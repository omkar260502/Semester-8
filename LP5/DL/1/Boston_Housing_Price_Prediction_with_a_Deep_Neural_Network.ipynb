{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "**Linear regression by using Deep Neural network**: Implement Boston housing price prediction problem by Linear regression using Deep Neural network. Use Boston House price prediction dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOTzZhdR9zZt"
   },
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "_F2vhnLJ3lm6"
   },
   "outputs": [],
   "source": [
    "# Data analysis and visualization\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Preprocessing and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhqBbskr-iag"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ILfTB4Ow3OAU",
    "outputId": "96cf0938-0070-41b5-d59d-d32f2fda7e6e"
   },
   "outputs": [],
   "source": [
    "(X_train , y_train), (X_test , y_test) = tf.keras.datasets.boston_housing.load_data(\n",
    "                                            path = 'boston_housing_npz',\n",
    "                                            test_split = 0.2,\n",
    "                                            seed = 42\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHPrTJsyHKSO"
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iFxpm9uu-mcs"
   },
   "source": [
    "## Initial Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lmgYsStF3vyi",
    "outputId": "d9d7c9ad-7e0a-4d0e-9c71-e20775ea9dc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((404, 13), numpy.ndarray),\n",
       " ((102, 13), numpy.ndarray),\n",
       " ((404,), numpy.ndarray),\n",
       " ((102,), numpy.ndarray))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the data shape and type\n",
    "(X_train.shape, type(X_train)), (X_test.shape, type(X_test)), (y_train.shape, type(y_train)), (y_test.shape, type(y_test)),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "-QVH6Dcl37Dl",
    "outputId": "860b6d19-12bf-4db1-a0bb-f579a0e671f0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.09178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.510</td>\n",
       "      <td>6.416</td>\n",
       "      <td>84.1</td>\n",
       "      <td>2.6463</td>\n",
       "      <td>5.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>16.6</td>\n",
       "      <td>395.50</td>\n",
       "      <td>9.04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.05644</td>\n",
       "      <td>40.0</td>\n",
       "      <td>6.41</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.447</td>\n",
       "      <td>6.758</td>\n",
       "      <td>32.9</td>\n",
       "      <td>4.0776</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>396.90</td>\n",
       "      <td>3.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.10574</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.74</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.609</td>\n",
       "      <td>5.983</td>\n",
       "      <td>98.8</td>\n",
       "      <td>1.8681</td>\n",
       "      <td>4.0</td>\n",
       "      <td>711.0</td>\n",
       "      <td>20.1</td>\n",
       "      <td>390.11</td>\n",
       "      <td>18.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.09164</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413</td>\n",
       "      <td>6.065</td>\n",
       "      <td>7.8</td>\n",
       "      <td>5.2873</td>\n",
       "      <td>4.0</td>\n",
       "      <td>305.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>390.91</td>\n",
       "      <td>5.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.09017</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.713</td>\n",
       "      <td>6.297</td>\n",
       "      <td>91.8</td>\n",
       "      <td>2.3682</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>385.09</td>\n",
       "      <td>17.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.10153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.437</td>\n",
       "      <td>6.279</td>\n",
       "      <td>74.5</td>\n",
       "      <td>4.0522</td>\n",
       "      <td>5.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>373.66</td>\n",
       "      <td>11.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.31827</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.90</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.544</td>\n",
       "      <td>5.914</td>\n",
       "      <td>83.2</td>\n",
       "      <td>3.9986</td>\n",
       "      <td>4.0</td>\n",
       "      <td>304.0</td>\n",
       "      <td>18.4</td>\n",
       "      <td>390.70</td>\n",
       "      <td>18.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.29090</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21.89</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.624</td>\n",
       "      <td>6.174</td>\n",
       "      <td>93.6</td>\n",
       "      <td>1.6119</td>\n",
       "      <td>4.0</td>\n",
       "      <td>437.0</td>\n",
       "      <td>21.2</td>\n",
       "      <td>388.08</td>\n",
       "      <td>24.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.03841</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.532</td>\n",
       "      <td>6.229</td>\n",
       "      <td>90.7</td>\n",
       "      <td>3.0993</td>\n",
       "      <td>24.0</td>\n",
       "      <td>666.0</td>\n",
       "      <td>20.2</td>\n",
       "      <td>395.33</td>\n",
       "      <td>12.87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.22438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.69</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.585</td>\n",
       "      <td>6.027</td>\n",
       "      <td>79.7</td>\n",
       "      <td>2.4982</td>\n",
       "      <td>6.0</td>\n",
       "      <td>391.0</td>\n",
       "      <td>19.2</td>\n",
       "      <td>396.90</td>\n",
       "      <td>14.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1      2    3      4      5     6       7     8      9     10  \\\n",
       "0  0.09178   0.0   4.05  0.0  0.510  6.416  84.1  2.6463   5.0  296.0  16.6   \n",
       "1  0.05644  40.0   6.41  1.0  0.447  6.758  32.9  4.0776   4.0  254.0  17.6   \n",
       "2  0.10574   0.0  27.74  0.0  0.609  5.983  98.8  1.8681   4.0  711.0  20.1   \n",
       "3  0.09164   0.0  10.81  0.0  0.413  6.065   7.8  5.2873   4.0  305.0  19.2   \n",
       "4  5.09017   0.0  18.10  0.0  0.713  6.297  91.8  2.3682  24.0  666.0  20.2   \n",
       "5  0.10153   0.0  12.83  0.0  0.437  6.279  74.5  4.0522   5.0  398.0  18.7   \n",
       "6  0.31827   0.0   9.90  0.0  0.544  5.914  83.2  3.9986   4.0  304.0  18.4   \n",
       "7  0.29090   0.0  21.89  0.0  0.624  6.174  93.6  1.6119   4.0  437.0  21.2   \n",
       "8  4.03841   0.0  18.10  0.0  0.532  6.229  90.7  3.0993  24.0  666.0  20.2   \n",
       "9  0.22438   0.0   9.69  0.0  0.585  6.027  79.7  2.4982   6.0  391.0  19.2   \n",
       "\n",
       "       11     12  \n",
       "0  395.50   9.04  \n",
       "1  396.90   3.53  \n",
       "2  390.11  18.07  \n",
       "3  390.91   5.52  \n",
       "4  385.09  17.27  \n",
       "5  373.66  11.97  \n",
       "6  390.70  18.33  \n",
       "7  388.08  24.16  \n",
       "8  395.33  12.87  \n",
       "9  396.90  14.33  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting Data to DataFrame \n",
    "X_train_df = pd.DataFrame(X_train)\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "\n",
    "# Preview the training data\n",
    "X_train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NtocpgHS6zUg",
    "outputId": "874e6070-2117-49b5-968e-44f980f9219e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404 entries, 0 to 403\n",
      "Data columns (total 13 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       404 non-null    float64\n",
      " 1   1       404 non-null    float64\n",
      " 2   2       404 non-null    float64\n",
      " 3   3       404 non-null    float64\n",
      " 4   4       404 non-null    float64\n",
      " 5   5       404 non-null    float64\n",
      " 6   6       404 non-null    float64\n",
      " 7   7       404 non-null    float64\n",
      " 8   8       404 non-null    float64\n",
      " 9   9       404 non-null    float64\n",
      " 10  10      404 non-null    float64\n",
      " 11  11      404 non-null    float64\n",
      " 12  12      404 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 41.2 KB\n",
      "________________________________________\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 404 entries, 0 to 403\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       404 non-null    float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 3.3 KB\n"
     ]
    }
   ],
   "source": [
    "# View summary of datasets\n",
    "X_train_df.info()\n",
    "print('_'*40)\n",
    "y_train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "id": "CeeFStx68vMP",
    "outputId": "887bb519-8460-4c38-f52a-e7c65e66a221"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.789989</td>\n",
       "      <td>11.568069</td>\n",
       "      <td>11.214059</td>\n",
       "      <td>0.069307</td>\n",
       "      <td>0.554524</td>\n",
       "      <td>6.284824</td>\n",
       "      <td>69.119307</td>\n",
       "      <td>3.792258</td>\n",
       "      <td>9.660891</td>\n",
       "      <td>408.960396</td>\n",
       "      <td>18.481931</td>\n",
       "      <td>356.293020</td>\n",
       "      <td>12.825520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.132761</td>\n",
       "      <td>24.269648</td>\n",
       "      <td>6.925462</td>\n",
       "      <td>0.254290</td>\n",
       "      <td>0.116408</td>\n",
       "      <td>0.723759</td>\n",
       "      <td>28.034606</td>\n",
       "      <td>2.142651</td>\n",
       "      <td>8.736073</td>\n",
       "      <td>169.685166</td>\n",
       "      <td>2.157322</td>\n",
       "      <td>92.058615</td>\n",
       "      <td>7.308772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.006320</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.385000</td>\n",
       "      <td>3.561000</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>1.137000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>12.600000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>1.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.081960</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.190000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.452000</td>\n",
       "      <td>5.878750</td>\n",
       "      <td>45.475000</td>\n",
       "      <td>2.097050</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>281.000000</td>\n",
       "      <td>17.400000</td>\n",
       "      <td>375.085000</td>\n",
       "      <td>7.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.262660</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.690000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538000</td>\n",
       "      <td>6.210000</td>\n",
       "      <td>77.500000</td>\n",
       "      <td>3.167500</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>330.000000</td>\n",
       "      <td>19.100000</td>\n",
       "      <td>391.305000</td>\n",
       "      <td>11.560000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.717875</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>18.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.624000</td>\n",
       "      <td>6.620500</td>\n",
       "      <td>94.425000</td>\n",
       "      <td>5.118000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>666.000000</td>\n",
       "      <td>20.200000</td>\n",
       "      <td>395.810000</td>\n",
       "      <td>17.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>88.976200</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>27.740000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.871000</td>\n",
       "      <td>8.780000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>12.126500</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>711.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>396.900000</td>\n",
       "      <td>37.970000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean     3.789989   11.568069   11.214059    0.069307    0.554524    6.284824   \n",
       "std      9.132761   24.269648    6.925462    0.254290    0.116408    0.723759   \n",
       "min      0.006320    0.000000    0.460000    0.000000    0.385000    3.561000   \n",
       "25%      0.081960    0.000000    5.190000    0.000000    0.452000    5.878750   \n",
       "50%      0.262660    0.000000    9.690000    0.000000    0.538000    6.210000   \n",
       "75%      3.717875   12.500000   18.100000    0.000000    0.624000    6.620500   \n",
       "max     88.976200  100.000000   27.740000    1.000000    0.871000    8.780000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean    69.119307    3.792258    9.660891  408.960396   18.481931  356.293020   \n",
       "std     28.034606    2.142651    8.736073  169.685166    2.157322   92.058615   \n",
       "min      2.900000    1.137000    1.000000  187.000000   12.600000    0.320000   \n",
       "25%     45.475000    2.097050    4.000000  281.000000   17.400000  375.085000   \n",
       "50%     77.500000    3.167500    5.000000  330.000000   19.100000  391.305000   \n",
       "75%     94.425000    5.118000   24.000000  666.000000   20.200000  395.810000   \n",
       "max    100.000000   12.126500   24.000000  711.000000   22.000000  396.900000   \n",
       "\n",
       "               12  \n",
       "count  404.000000  \n",
       "mean    12.825520  \n",
       "std      7.308772  \n",
       "min      1.920000  \n",
       "25%      7.092500  \n",
       "50%     11.560000  \n",
       "75%     17.167500  \n",
       "max     37.970000  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of numerical feature values across the samples\n",
    "X_train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0S3DE2_c_8j2"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 344
    },
    "id": "JEXlXchREDC6",
    "outputId": "891b630d-c9f4-4c11-9f09-dbbf883324d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "      <td>404.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.042528</td>\n",
       "      <td>0.115681</td>\n",
       "      <td>0.394210</td>\n",
       "      <td>0.348815</td>\n",
       "      <td>0.521905</td>\n",
       "      <td>0.681970</td>\n",
       "      <td>0.241618</td>\n",
       "      <td>0.376560</td>\n",
       "      <td>0.423589</td>\n",
       "      <td>0.625737</td>\n",
       "      <td>0.897607</td>\n",
       "      <td>0.302511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.102650</td>\n",
       "      <td>0.242696</td>\n",
       "      <td>0.253866</td>\n",
       "      <td>0.239522</td>\n",
       "      <td>0.138678</td>\n",
       "      <td>0.288719</td>\n",
       "      <td>0.194973</td>\n",
       "      <td>0.379829</td>\n",
       "      <td>0.323827</td>\n",
       "      <td>0.229502</td>\n",
       "      <td>0.232131</td>\n",
       "      <td>0.202740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.173387</td>\n",
       "      <td>0.137860</td>\n",
       "      <td>0.444098</td>\n",
       "      <td>0.438466</td>\n",
       "      <td>0.087361</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.179389</td>\n",
       "      <td>0.510638</td>\n",
       "      <td>0.944992</td>\n",
       "      <td>0.143481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.002881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.338343</td>\n",
       "      <td>0.314815</td>\n",
       "      <td>0.507569</td>\n",
       "      <td>0.768280</td>\n",
       "      <td>0.184767</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.272901</td>\n",
       "      <td>0.691489</td>\n",
       "      <td>0.985892</td>\n",
       "      <td>0.267406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.041717</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.646628</td>\n",
       "      <td>0.491770</td>\n",
       "      <td>0.586223</td>\n",
       "      <td>0.942585</td>\n",
       "      <td>0.362255</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.914122</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.997252</td>\n",
       "      <td>0.422954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000   \n",
       "mean     0.042528    0.115681    0.394210    0.348815    0.521905    0.681970   \n",
       "std      0.102650    0.242696    0.253866    0.239522    0.138678    0.288719   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000850    0.000000    0.173387    0.137860    0.444098    0.438466   \n",
       "50%      0.002881    0.000000    0.338343    0.314815    0.507569    0.768280   \n",
       "75%      0.041717    0.125000    0.646628    0.491770    0.586223    0.942585   \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "               6           7           8           9           10          11  \n",
       "count  404.000000  404.000000  404.000000  404.000000  404.000000  404.000000  \n",
       "mean     0.241618    0.376560    0.423589    0.625737    0.897607    0.302511  \n",
       "std      0.194973    0.379829    0.323827    0.229502    0.232131    0.202740  \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000  \n",
       "25%      0.087361    0.130435    0.179389    0.510638    0.944992    0.143481  \n",
       "50%      0.184767    0.173913    0.272901    0.691489    0.985892    0.267406  \n",
       "75%      0.362255    1.000000    0.914122    0.808511    0.997252    0.422954  \n",
       "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create column transformer\n",
    "ct = make_column_transformer(\n",
    "    (MinMaxScaler(), [0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11, 12])\n",
    ")\n",
    "\n",
    "# Normalization and data type change\n",
    "X_train = ct.fit_transform(X_train).astype('float32')\n",
    "X_test = ct.transform(X_test).astype('float32')\n",
    "y_train = y_train.astype('float32')\n",
    "y_test = y_test.astype('float32')\n",
    "\n",
    "# Distribution of X_train feature values after normalization\n",
    "pd.DataFrame(X_train).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwbgGLK9FyRG"
   },
   "source": [
    "# Model, Predict, Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0yG6aN-_RDxF",
    "outputId": "dcf22498-58a2-405e-f110-e0def71cd652"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((363, 12), (41, 12), (363,), (41,))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reserve data for validation\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rXXjkEnchKQ0"
   },
   "source": [
    "## Creating the Model and Optimizing the Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt3qQohTiNK8"
   },
   "source": [
    "learning rate = 0.01,\n",
    "batch_size = 32,\n",
    "dense_layers = 2,\n",
    "hidden_units for Dense_1 layer= 10,\n",
    "hidden_units for Dense_2 layer = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rxuLioVJHXep",
    "outputId": "db41144b-fcda-4412-d658-e8c7b4b48129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 32ms/step - loss: 379.6244 - mse: 379.6244 - val_loss: 100.5909 - val_mse: 100.5909\n",
      "Epoch 2/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 69.5007 - mse: 69.5007 - val_loss: 63.1661 - val_mse: 63.1661\n",
      "Epoch 3/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 48.2531 - mse: 48.2531 - val_loss: 53.4647 - val_mse: 53.4647\n",
      "Epoch 4/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 42.4763 - mse: 42.4763 - val_loss: 47.7719 - val_mse: 47.7719\n",
      "Epoch 5/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 38.8984 - mse: 38.8984 - val_loss: 42.6301 - val_mse: 42.6301\n",
      "Epoch 6/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 35.9923 - mse: 35.9923 - val_loss: 37.1396 - val_mse: 37.1396\n",
      "Epoch 7/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 33.0611 - mse: 33.0611 - val_loss: 32.3117 - val_mse: 32.3117\n",
      "Epoch 8/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 30.6548 - mse: 30.6548 - val_loss: 28.7234 - val_mse: 28.7234\n",
      "Epoch 9/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 28.9628 - mse: 28.9628 - val_loss: 26.2408 - val_mse: 26.2408\n",
      "Epoch 10/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 27.7799 - mse: 27.7799 - val_loss: 24.3911 - val_mse: 24.3911\n",
      "Epoch 11/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 27.0834 - mse: 27.0834 - val_loss: 22.9319 - val_mse: 22.9319\n",
      "Epoch 12/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 26.6332 - mse: 26.6332 - val_loss: 21.7267 - val_mse: 21.7267\n",
      "Epoch 13/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 26.1111 - mse: 26.1111 - val_loss: 20.7510 - val_mse: 20.7510\n",
      "Epoch 14/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25.6743 - mse: 25.6743 - val_loss: 19.9615 - val_mse: 19.9615\n",
      "Epoch 15/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 25.4433 - mse: 25.4433 - val_loss: 19.2497 - val_mse: 19.2497\n",
      "Epoch 16/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 25.0438 - mse: 25.0438 - val_loss: 18.9868 - val_mse: 18.9868\n",
      "Epoch 17/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24.8239 - mse: 24.8239 - val_loss: 18.6175 - val_mse: 18.6175\n",
      "Epoch 18/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24.6920 - mse: 24.6920 - val_loss: 18.1209 - val_mse: 18.1209\n",
      "Epoch 19/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 24.2442 - mse: 24.2442 - val_loss: 17.6045 - val_mse: 17.6045\n",
      "Epoch 20/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23.9032 - mse: 23.9032 - val_loss: 17.3070 - val_mse: 17.3070\n",
      "Epoch 21/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 23.6196 - mse: 23.6196 - val_loss: 17.1061 - val_mse: 17.1061\n",
      "Epoch 22/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23.3107 - mse: 23.3107 - val_loss: 17.0511 - val_mse: 17.0511\n",
      "Epoch 23/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 23.0681 - mse: 23.0681 - val_loss: 16.8544 - val_mse: 16.8544\n",
      "Epoch 24/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 22.9731 - mse: 22.9731 - val_loss: 16.8227 - val_mse: 16.8227\n",
      "Epoch 25/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 22.6766 - mse: 22.6766 - val_loss: 17.0509 - val_mse: 17.0509\n",
      "Epoch 26/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 22.5081 - mse: 22.5081 - val_loss: 16.9686 - val_mse: 16.9686\n",
      "Epoch 27/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 22.3025 - mse: 22.3025 - val_loss: 17.0156 - val_mse: 17.0156\n",
      "Epoch 28/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 22.0691 - mse: 22.0691 - val_loss: 17.3753 - val_mse: 17.3753\n",
      "Epoch 29/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 21.9070 - mse: 21.9070 - val_loss: 17.9596 - val_mse: 17.9596\n",
      "Epoch 30/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.8558 - mse: 21.8558 - val_loss: 18.4322 - val_mse: 18.4322\n",
      "Epoch 31/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.8655 - mse: 21.8655 - val_loss: 19.6357 - val_mse: 19.6357\n",
      "Epoch 32/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 22.1712 - mse: 22.1712 - val_loss: 19.6037 - val_mse: 19.6037\n",
      "Epoch 33/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.6966 - mse: 21.6966 - val_loss: 19.8508 - val_mse: 19.8508\n",
      "Epoch 34/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 21.5912 - mse: 21.5912 - val_loss: 20.4139 - val_mse: 20.4139\n",
      "Epoch 35/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.4305 - mse: 21.4305 - val_loss: 20.8007 - val_mse: 20.8007\n",
      "Epoch 36/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 21.3431 - mse: 21.3431 - val_loss: 20.4769 - val_mse: 20.4769\n",
      "Epoch 37/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 20.9400 - mse: 20.9400 - val_loss: 20.6909 - val_mse: 20.6909\n",
      "Epoch 38/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 20.7084 - mse: 20.7084 - val_loss: 21.0832 - val_mse: 21.0832\n",
      "Epoch 39/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 20.5142 - mse: 20.5142 - val_loss: 21.9822 - val_mse: 21.9822\n",
      "Epoch 40/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 20.3613 - mse: 20.3613 - val_loss: 22.8254 - val_mse: 22.8254\n",
      "Epoch 41/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 20.4129 - mse: 20.4129 - val_loss: 22.3909 - val_mse: 22.3909\n",
      "Epoch 42/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.9555 - mse: 19.9555 - val_loss: 22.5640 - val_mse: 22.5640\n",
      "Epoch 43/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.6543 - mse: 19.6543 - val_loss: 22.9816 - val_mse: 22.9816\n",
      "Epoch 44/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.7987 - mse: 19.7987 - val_loss: 22.4908 - val_mse: 22.4908\n",
      "Epoch 45/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.2155 - mse: 19.2155 - val_loss: 23.0613 - val_mse: 23.0613\n",
      "Epoch 46/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 19.3839 - mse: 19.3839 - val_loss: 22.6324 - val_mse: 22.6324\n",
      "Epoch 47/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 19.0229 - mse: 19.0229 - val_loss: 22.7062 - val_mse: 22.7062\n",
      "Epoch 48/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.7888 - mse: 18.7888 - val_loss: 22.7878 - val_mse: 22.7878\n",
      "Epoch 49/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 18.7710 - mse: 18.7710 - val_loss: 21.9603 - val_mse: 21.9603\n",
      "Epoch 50/50\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 18.3125 - mse: 18.3125 - val_loss: 22.3188 - val_mse: 22.3188\n"
     ]
    }
   ],
   "source": [
    "# Set random seed\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Building the model\n",
    "model = tf.keras.Sequential([\n",
    "  tf.keras.layers.Dense(units=10, activation='relu', input_shape=(X_train.shape[1],), name='Dense_1'),\n",
    "  tf.keras.layers.Dense(units=100, activation='relu', name='Dense_2'),\n",
    "  tf.keras.layers.Dense(units=1, name='Prediction')\n",
    "])\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(\n",
    "    loss = tf.keras.losses.mean_squared_error,\n",
    "    optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.01),\n",
    "    metrics = ['mse']\n",
    ")\n",
    "\n",
    "# Training the model\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=32,\n",
    "    epochs=50,\n",
    "    validation_data=(X_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VasQEdJRe9NK"
   },
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1t-7HcflKgGV",
    "outputId": "71e123d7-73d3-4d6e-97d9-3a6aa6774076"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22.235537, 24.89756)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Preview the mean value of training and validation data\n",
    "y_train.mean(), y_val.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4F5mom8FfACb",
    "outputId": "1577826d-50d7-47a8-ab8c-d423d52be46e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on Test data \n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluation on Test data \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m loss, mse \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(\u001b[43mx_val\u001b[49m, y_val, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel loss on test set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel mean squared error on test set: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(mse)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_val' is not defined"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "print(\"Evaluation on Test data \\n\")\n",
    "loss, mse = model.evaluate(x_val, y_val, batch_size=32)\n",
    "print(f\"\\nModel loss on test set: {loss}\")\n",
    "print(f\"Model mean squared error on test set: {(mse):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "1CD_HeZ1LFZ5",
    "outputId": "4caf3820-2be3-421d-9826-600baf83d452"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAze0lEQVR4nO3deZxcVZn/8c9zb+29ZunsIQkhGJaGAAmLkV1FUEFwBJXRwKgoMizCMMRlfiwDbsy4wI+BH2oEEYSAghlBFiESg0o2swCJSQgJ2dOdpNfa731+f9ybpoEEO0lXV5J63q9Xvarq3Fqe2+n0t845t+4RVcUYY4wBcMpdgDHGmH2HhYIxxpguFgrGGGO6WCgYY4zpYqFgjDGmi4WCMcaYLhYKxhhjulgomAOKiKwWkQ+W6b2PF5GnRKRFRLaJyBwRubQctRizpywUjOkFInIS8ALwInAIMAC4HDh7D1/P7b3qjOk5CwVTEUQkLiI/EpEN4eVHIhIPtw0Ukd91+4T/JxFxwm03iMh6EWkXkb+LyJm7eIvbgftV9Xuq2qyB+ap6Yfg6l4jI7HfUpCJySHj7PhG5O+xpdAL/JiKbuoeDiJwvIovD246ITBWR10Vkq4hMF5H+4baEiPwybG8RkbkiMriXf6TmAGWhYCrFN4ETgQnA0cDxwLfCbdcB64AGYDDwDUBF5H3AvwKTVLUGOAtY/c4XFpEUcBLw2F7W+FngNqAG+DHQCZzxju0PhbevBD4BnAoMA7YDd4XbpgB1wEiCHstXgMxe1mYqhIWCqRQXA7eo6hZVbQJuBj4XbisAQ4FRqlpQ1T9pcFIwD4gDh4tIVFVXq+rrO3ntfgT/lzbuZY2/VdWXVNVX1SzwK+AzACJSA5wTtkHwh/6bqrpOVXPATcA/iUgk3J8BwCGq6oU9lra9rM1UCAsFUymGAWu63V8TtkEw9LMSeFZEVonIVABVXQlcQ/AHd4uIPCwiw3i37YBPECx7Y+077j8EXBAOc10ALFDVHfswCng8HB5qAZYShNhg4AHgGeDhcKjs+yIS3cvaTIWwUDCVYgPBH9IdDgrbUNV2Vb1OVQ8GzgWu3TF3oKoPqeoHwucq8L13vrCqpoG/AJ98j/fvBFI77ojIkJ085m2nLFbV1wjC62zePnQEQYCcrar13S4JVV0f9nZuVtXDgfcDHwM+/x61GdPFQsEciKLhZOuOS4Rg2OVbItIgIgOB/wP8EkBEPiYih4iIAK0En7h9EXmfiJwRflLPEozL+7t4z38HLhGR60VkQPi6R4vIw+H2RcARIjJBRBIEvY+eeAi4GjgFeLRb+z3AbSIyKnyvBhE5L7x9uog0hpPUbQTDSbuq25i3sVAwB6KnCP6A77jcBNwKzAMWA0uABWEbwDjgD0AHwSf+/1HVmQTzCd8FmoFNwCDg6zt7Q1X9M8Gk8BnAKhHZBtwb1oKqLgduCd9nBTB7Z6+zE78imEx+QVWbu7X/GJhBMOTVDvwVOCHcNoRg0ruNYFjpRYIhJWP+IbFFdowxxuxgPQVjjDFdLBSMMcZ0sVAwxhjTxULBGGNMl0i5C9gbAwcO1NGjR5e7DGOM2a/Mnz+/WVUbdrZtvw6F0aNHM2/evHKXYYwx+xURWbOrbTZ8ZIwxpouFgjHGmC4WCsYYY7rs13MKxpjKUigUWLduHdlsttyl7BcSiQQjRowgGu35SXItFIwx+41169ZRU1PD6NGjCc5faHZFVdm6dSvr1q1jzJgxPX6eDR8ZY/Yb2WyWAQMGWCD0gIgwYMCA3e5VWSgYY/YrFgg9tyc/q5KFgoiMFJGZIvKaiLwqIleH7TeFC6EvDC/ndHvO10VkZbhA+lmlqu2N1+byl59+je1Ne7t6ojHGHFhK2VMoAteFqz+dCFwhIoeH236oqhPCy1MA4bZPA0cAHwH+J1wkpNdtf/NVTlo3jZYtb5bi5Y0xB7Dq6upyl1BSJQsFVd2oqgvC2+0Ei30Mf4+nnAc8rKo5VX2DYM3c40tRmxuvAiCf6SzFyxtjzH6rT+YURGQ0cAzwctj0ryKyWESmiUi/sG04b1+4fB07CRERuUxE5onIvKampj2qJ5oIkr6Qs1AwxuwZVeX666/nyCOPpLGxkUceeQSAjRs3csoppzBhwgSOPPJI/vSnP+F5HpdccknXY3/4wx+WufpdK/khqSJSDfwauEZV20TkbuA/CRYp/0/gv4F/6enrqeq9BMscMnHixD1aNi6aCHoKxayFgjH7q5v/91Ve29DWq695+LBabvz4ET167G9+8xsWLlzIokWLaG5uZtKkSZxyyik89NBDnHXWWXzzm9/E8zzS6TQLFy5k/fr1vPLKKwC0tLT0at29qaQ9BRGJEgTCg6r6GwBV3ayqnqr6wE94a4hoPTCy29NHhG29LhaGgmc9BWPMHpo9ezaf+cxncF2XwYMHc+qppzJ37lwmTZrEz3/+c2666SaWLFlCTU0NBx98MKtWreLKK6/k6aefpra2ttzl71LJegoSHAv1M2Cpqv6gW/tQVd1x2M/5wCvh7RnAQyLyA2AYwWLqc0pRWzQZDB95uXQpXt4Y0wd6+om+r51yyinMmjWLJ598kksuuYRrr72Wz3/+8yxatIhnnnmGe+65h+nTpzNt2rRyl7pTpewpTAY+B5zxjsNPvy8iS0RkMXA68DUAVX0VmA68BjwNXKGqXikKiyeDnoLmLRSMMXvm5JNP5pFHHsHzPJqampg1axbHH388a9asYfDgwXzpS1/ii1/8IgsWLKC5uRnf9/nkJz/JrbfeyoIFC8pd/i6VrKegqrOBnX1z4qn3eM5twG2lqmmHRCroKWjBQsEYs2fOP/98/vKXv3D00UcjInz/+99nyJAh3H///dx+++1Eo1Gqq6v5xS9+wfr167n00kvxfR+A73znO2Wuftcq8txHyVQNAJrPlLkSY8z+pqOjAwi+LXz77bdz++23v237lClTmDJlyruety/3DrqryNNcOK5LVqNQsIlmY4zpriJDASArcZyC9RSMMaa7ig2FHHGkaKFgjDHdVWwo5CWO69lCHcYY013lhoKTwLFQMMaYt6noUIh4NnxkjDHdVWwoFJ0EUespGGPM21RuKLgJomqhYIwx3VVsKHhukpifK3cZxpj9zOrVqxk/fjyXXHIJhx56KBdffDF/+MMfmDx5MuPGjWPOnDm8+OKLTJgwgQkTJnDMMcfQ3t4OwO23386kSZM46qijuPHGG8u8JztXkd9oBvAjCeLWUzBm//X7qbBpSe++5pBGOPu7//BhK1eu5NFHH2XatGlMmjSJhx56iNmzZzNjxgy+/e1v43ked911F5MnT6ajo4NEIsGzzz7LihUrmDNnDqrKueeey6xZszjllFN6dx/2UsX2FDSSJI71FIwxu2/MmDE0NjbiOA5HHHEEZ555JiJCY2Mjq1evZvLkyVx77bXccccdtLS0EIlEePbZZ3n22Wc55phjOPbYY1m2bBkrVqwo9668SwX3FFIkNF/uMowxe6oHn+hLJR6Pd912HKfrvuM4FItFpk6dykc/+lGeeuopJk+ezDPPPIOq8vWvf50vf/nL5Sq7Ryq2p0AsRVwKeMViuSsxxhxgXn/9dRobG7nhhhuYNGkSy5Yt46yzzmLatGldJ9Rbv349W7ZsKXOl71axPQWJJgHIpNupru33Dx5tjDE996Mf/YiZM2d2DS+dffbZxONxli5dykknnQRAdXU1v/zlLxk0aFCZq327yg2FWAqAbLrDQsEY02OjR4/uWmsZ4L777tvltne6+uqrufrqq0tZ3l6r2OEjJwyFfMZOn22MMTtUcCgES3Lmsx1lrsQYY/YdFRsKkUQwp2A9BWOMeUvlhkI8WJKzkLGegjHG7FCxoRBNBsNHxZz1FIwxZofKDYVEEAqehYIxxnSp2FCIJaoB8HLpMldijDH7jooNhXgq6Cn4eQsFY0xpVFdX73Lb6tWrOfLII/uwmp6p3FBIBv9YWrBQMMaYHSr2G83JVBgK1lMwZr/0vTnfY9m2Zb36muP7j+eG42/Y5fapU6cycuRIrrjiCgBuuukmIpEIM2fOZPv27RQKBW699VbOO++83XrfbDbL5Zdfzrx584hEIvzgBz/g9NNP59VXX+XSSy8ln8/j+z6//vWvGTZsGBdeeCHr1q3D8zz+4z/+g4suumiv9ru7ig2FSDRGXiNgPQVjTA9ddNFFXHPNNV2hMH36dJ555hmuuuoqamtraW5u5sQTT+Tcc89FRHr8unfddRciwpIlS1i2bBkf/vCHWb58Offccw9XX301F198Mfl8Hs/zeOqppxg2bBhPPvkkAK2trb26jxUbCgBZiSOFTLnLMMbsgff6RF8qxxxzDFu2bGHDhg00NTXRr18/hgwZwte+9jVmzZqF4zisX7+ezZs3M2TIkB6/7uzZs7nyyisBGD9+PKNGjWL58uWcdNJJ3Hbbbaxbt44LLriAcePG0djYyHXXXccNN9zAxz72MU4++eRe3ceKnVMAyBLHKVooGGN67lOf+hSPPfYYjzzyCBdddBEPPvggTU1NzJ8/n4ULFzJ48GCy2d5Z1fGzn/0sM2bMIJlMcs455/DCCy9w6KGHsmDBAhobG/nWt77FLbfc0ivvtUNF9xTyEsfxbElOY0zPXXTRRXzpS1+iubmZF198kenTpzNo0CCi0SgzZ85kzZo1u/2aJ598Mg8++CBnnHEGy5cv58033+R973sfq1at4uCDD+aqq67izTffZPHixYwfP57+/fvzz//8z9TX1/PTn/60V/ev4kPBtZ6CMWY3HHHEEbS3tzN8+HCGDh3KxRdfzMc//nEaGxuZOHEi48eP3+3X/OpXv8rll19OY2MjkUiE++67j3g8zvTp03nggQeIRqMMGTKEb3zjG8ydO5frr78ex3GIRqPcfffdvbp/oqq9+oJ9aeLEiTpv3rw9fv7fbz2BvJui8esze7EqY0ypLF26lMMOO6zcZexXdvYzE5H5qjpxZ4+v6DmFgpsg4tvwkTHG7FCy4SMRGQn8AhgMKHCvqv5YRPoDjwCjgdXAhaq6XYLjt34MnAOkgUtUdUGp6gMoukkSxfZSvoUxpsItWbKEz33uc29ri8fjvPzyy2Wq6L2Vck6hCFynqgtEpAaYLyLPAZcAz6vqd0VkKjAVuAE4GxgXXk4A7g6vS8ZzE8TUegrGmNJpbGxk4cKF5S6jx0o2fKSqG3d80lfVdmApMBw4D7g/fNj9wCfC2+cBv9DAX4F6ERlaqvoAPDdJzM+V8i2MMWa/0idzCiIyGjgGeBkYrKobw02bCIaXIAiMtd2eti5se+drXSYi80RkXlNT017VpZEEcSwUjDFmh5KHgohUA78GrlHVtu7bNDj0abcOf1LVe1V1oqpObGho2KvaNJIkacNHxhjTpaShICJRgkB4UFV/EzZv3jEsFF5vCdvXAyO7PX1E2FYyGqsiIQV8zyvl2xhjzH6jZKEQHk30M2Cpqv6g26YZwJTw9hTgt93aPy+BE4HWbsNMpakxmgQga+s0G2NK4L3WU9hXlfLoo8nA54AlIrIwbPsG8F1guoh8AVgDXBhue4rgcNSVBIekXlrC2gCQWAqAbLqDVHVdqd/OGGP2eSULBVWdDezq3LFn7uTxClxRqnp2Zkco5KynYMx+Z9O3v01uae+upxA/bDxDvvGNXW7vzfUU/vjHP3LjjTdSX1/PkiVLuPDCC2lsbOTHP/4xmUyGJ554grFjx/Loo49y880347oudXV1zJo1C8/zmDp1Kn/84x/J5XJcccUVfPnLX+6Vn0FFf6PZDUMhn+kscyXGmP3BRRddxPTp07vuT58+nSlTpvD444+zYMECZs6cyXXXXUdPTx+0aNEi7rnnHpYuXcoDDzzA8uXLmTNnDl/84he58847Abjlllt45plnWLRoETNmzADgZz/7GXV1dcydO5e5c+fyk5/8hDfeeKNX9rGiT4jnxoN1mvPWUzBmv/Nen+hLpbfXU5g0aRJDhwZfxxo7diwf/vCHgeALbzNnBudkmzx5MpdccgkXXnghF1xwAQDPPvssixcv5rHHHgOChXZWrFjBmDFj9nofKzoUIvGgp1DIWk/BGNMzO9ZT2LRp07vWU4hGo4wePbrH6ynE4/Gu247jdN13HIdisQjAPffcw8svv8yTTz7Jcccdx/z581FV7rzzTs4666xe37+KHj6KJIMjA4oWCsaYHrrooot4+OGHeeyxx/jUpz5Fa2vrXq+n8F5ef/11TjjhBG655RYaGhpYu3YtZ511FnfffTeFQgGA5cuX09nZO3/HKrqnEEsEoeDlLRSMMT1TivUU3sv111/PihUrUFXOPPNMjj76aI466ihWr17Nsccei6rS0NDAE0880SvvV9HrKaxb+QojfjmZecd+l4nnXt6LlRljSsHWU9h9tp7Cboinwp5CLl3mSowxZt9Q2cNH4ZyC2vCRMaZEbD2F/Ugy7ClowXoKxuwvVJXgLDr7h3Kup7An0wMVPXwUiycoqAuFTLlLMcb0QCKRYOvWrXv0x67SqCpbt24lkUjs1vMquqcAkCWGWCgYs18YMWIE69atY2/XUqkUiUSCESNG7NZzKj4UchJHihYKxuwPotFor3xr1+xaRQ8fQRAKroWCMcYAFgrkJYHj2eprxhgDFgoUnDgRz3oKxhgDFgoUnAQR6ykYYwxgoUDRTRL1LRSMMQYsFPDcBDHNlbsMY4zZJ1gouEli1lMwxhjAQgE/kiCO9RSMMQYsFNBIkoQNHxljDGChANEUKcmhvl/uSowxpuwqPhQ0VgVALmtnSjXGmIoPBYkmAcimO8pciTHGlF/Fh4ITSwGQzVgoGGOMhUI8CIW8hYIxxlgouDELBWOM2aHiQyGSCCaaC1lbp9kYYyo+FKLxIBSKFgrGGGOhEE1WA1DM2SGpxhhT8aEQSwY9BS9nPQVjjLFQCHsKft56CsYYU/GhkNgRCtZTMMaY0oWCiEwTkS0i8kq3tptEZL2ILAwv53Tb9nURWSkifxeRs0pV1zvFU0EoaMF6CsYYU8qewn3AR3bS/kNVnRBengIQkcOBTwNHhM/5HxFxS1hbl3g8ia8CBVun2RhjShYKqjoL2NbDh58HPKyqOVV9A1gJHF+q2roTxyFDHLFQMMaYsswp/KuILA6Hl/qFbcOBtd0esy5sexcRuUxE5onIvKampl4pKCtxpGihYIwxfR0KdwNjgQnARuC/d/cFVPVeVZ2oqhMbGhp6paicxHEsFIwxpm9DQVU3q6qnqj7wE94aIloPjOz20BFhW5/ISxzXQsEYY/o2FERkaLe75wM7jkyaAXxaROIiMgYYB8zpq7oKTgLXs1AwxphIqV5YRH4FnAYMFJF1wI3AaSIyAVBgNfBlAFV9VUSmA68BReAKVfVKVds7FZwEET/bV29njDH7rB6FgohUARlV9UXkUGA88HtVLezqOar6mZ00/+w9Hn8bcFtP6ultBSdBsthajrc2xph9Sk+Hj2YBCREZDjwLfI7gewgHBM9NErOegjHG9DgURFXTwAXA/6jqpwi+aHZA8CIJYpordxnGGFN2PQ4FETkJuBh4Mmzrk28c9wU/kiSu1lMwxpiehsI1wNeBx8NJ4YOBmSWrqo9pJEnCegrGGNOziWZVfRF4EUBEHKBZVa8qZWF9SSNJEuRQ30ecij9xrDGmgvXoL6CIPCQiteFRSK8Ar4nI9aUtre9INIUrSj5vQ0jGmMrW04/Fh6tqG/AJ4PfAGIIjkA4MsRQA2bStqWCMqWw9DYWoiEQJQmFG+P0ELVlVfcwJQyGX6ShzJcYYU149DYX/R/AN5CpgloiMAtpKVVRf6wqF9AGzS8YYs0d6OtF8B3BHt6Y1InJ6aUrqe068CoB8xoaPjDGVracTzXUi8oMd6xiIyH8T9BoOCJF40FMoZC0UjDGVrafDR9OAduDC8NIG/LxURfW1SCLIt0LW1mk2xlS2np4ldayqfrLb/ZtFZGEJ6imLWLIaAC9nPQVjTGXraU8hIyIf2HFHRCYDB8wCBNFEEArFvIWCMaay9bSn8BXgFyJSF97fDkwpTUl9L54Mho98m1MwxlS4nh59tAg4WkRqw/ttInINsLiEtfWZeDh85OdtTsEYU9l260Q/qtoWfrMZ4NoS1FMWiVQQClo4YEbEjDFmj+zN2d+k16oos3giOCSVgvUUjDGVbW9C4cA5zYXrktEYYj0FY0yFe885BRFpZ+d//AVIlqSiMslKAilaKBhjKtt7hoKq1vRVIeWWI45jw0fGmApnK8qEck4c17OegjGmslkohAoSx/VskR1jTGWzUAgVnAQRCwVjTIWzUAgV3AQR30LBGFPZLBRCnpsk5ufKXYYxxpSVhULIcxPE1CaajTGVzUIh5EeSxNV6CsaYymahELJQMMYYC4UuGkmSIF/uMowxpqwsFHaIJomKRyFvvQVjTOWyUAhJLFhoJ5PuKHMlxhhTPhYKIYkFp8/OpdvLXIkxxpRPyUJBRKaJyBYReaVbW38ReU5EVoTX/cJ2EZE7RGSliCwWkWNLVdeuOF2hYD0FY0zlKmVP4T7gI+9omwo8r6rjgOfD+wBnA+PCy2XA3SWsizde+TO/v+VLdLZv62pz40Eo5G2dZmNMBStZKKjqLGDbO5rPA+4Pb98PfKJb+y808FegXkSGlqq2DYv/yuiHZvP6/Be62tx4MKeQz1pPwRhTufp6TmGwqm4Mb28CBoe3hwNruz1uXdj2LiJymYjME5F5TU1Ne1TE6OPPAGDzgj93tUXCUChaT8EYU8HKNtGsqsoeLOmpqveq6kRVndjQ0LBH7z304KNoTwm5V5d2tUUTYSjkbKEdY0zl6utQ2LxjWCi83hK2rwdGdnvciLCtJBzHYeuoelKrNnW1xZPVAHg2fGSMqWB9HQozgCnh7SnAb7u1fz48CulEoLXbMFNJ+IeOoWFzlky6DYDojlDIW0/BGFO5SnlI6q+AvwDvE5F1IvIF4LvAh0RkBfDB8D7AU8AqYCXwE+Crpaprh7rGCUR8WDn3eQDiyWD4SC0UjDEVLFKqF1bVz+xi05k7eawCV5Sqlp0ZdfwZdDCNTQteovHU80mkgp6ChYIxppJV7Deahx9yDB1JIbc0mGxOhMNHWrA1FYwxlatiQ8FxHLYeVEfy9Q0AuJEIOY1CwXoKxpjKVbGhAOAdOopBG7PkMsERRxmJ41goGGMqWEWHQk3jMcFk8/xgsjlHHClmy1yVMcaUT0WHwqhJpwOwcf5sAPISx/VsTsEYU7kqOhRGvm8inQkh+1ow2Zx3Erie9RSMMZWrokPBcRyaD6rtmmwuOAki1lMwxlSwig4FAG/cKBo2Zshn0hScOBHPluM0xlSuig+FmsYJRD1Y+bcXKLpJYr71FIwxlaviQ+GgSacBwWSz5yaJqfUUjDGVy0LhsBNIx4XMa6/iRxIWCsaYilbxoRBMNteQWLkBjSRJYKFgjKlcFR8KAMVDDqJhQ5q8xEhYT8EYU8EsFIDqxqOJebC9tZ2YFCkW8uUuyRhjysJCARgZTjanm7cBkEnb6mvGmMpkoQCMOvxEMjFwN7UAdJ0gzxhjKo2FAuC6EZpG1lC7KQiDXLqzzBUZY0x5WCiECuMOYsjmAgUf8lnrKRhjKpOFQqi68ShiRXg9naCQtZ6CMaYyWSiERkw8FYANbQkKGQsFY0xlslAIjTlyMpkYpLfHKOYsFIwxlclCIeS6EbYMTRJvdina8JExpkJZKHSTGTOUhiYh17Kx3KUYY0xZWCh0U3fMROJFyC3+dblLMcaYsrBQ6OagE88AILO2hZWLZpe5GmOM6XsWCt0c3HgyK48bwqgFMZb86sZyl2OMMX3OQqEbx3E47a5HaOonDH12A8sX/qncJRljTJ+yUHiHmvpBRG64hkQBVl5/BYV8ttwlGWNMn7FQ2IkPnHcZq05OMmZtgWe+MaXc5RhjTJ+xUNiFMZ+4ltePKDL2d4v58/Q7yl2OMcb0CQuFXWg8/UIaj1DWDRJi376HN5fNLXdJxhhTchYKuxCJxthwyGc49KQmRJUVV1xGJt1W7rKMMaakyhIKIrJaRJaIyEIRmRe29ReR50RkRXjdrxy1dXfYOVcwOKGs+vAwhq3PMnPKR2lpXl/usowxpmTK2VM4XVUnqOrE8P5U4HlVHQc8H94vq/qBQ1jc70N8NLWEVVPOYOSrzSw59yMs/evvy12aMcaUxL40fHQecH94+37gE+Ur5S39T/9XUpJjwEGD8O+8Gbfok//CtTx/1zfKXZoxxvS6coWCAs+KyHwRuSxsG6yqO85EtwkYXJ7S3u6QoyezNHo4I1c+yJGnXsC4x3/LxrF1DLvzcX73hbPpbN9W7hKNMabXlCsUPqCqxwJnA1eIyCndN6qqEgTHu4jIZSIyT0TmNTU19UGpkJ7wBYbrZpb88VEahh/CB389izcumMjYl1Yz5+NnsGqJnSfJGHNgKEsoqOr68HoL8DhwPLBZRIYChNdbdvHce1V1oqpObGho6JN6j/rQ59hIAwfPvo4lL/6GSDTGOd9+gNZvX0l1a57Wz36J5++ciu/7fVKPMcaUSp+HgohUiUjNjtvAh4FXgBnAjq8PTwF+29e17Uo0Fken/I6tzkAOf+Ff+OtD/4n6Pide8FWGP/Ywm0fXMeyu3/L0haeyZe3fy12uMcbssXL0FAYDs0VkETAHeFJVnwa+C3xIRFYAHwzv7zOGjRlPwzUvsrjq/Zy4/L+Ye8fF5LJpho09ig89MZu1//Ihhi9rZvV55/PSQz8od7nGGLNHJBi+3z9NnDhR582b16fv6XseL9/375y09qcsixzGwC9OZ+CQgwBY+beZrPm3axm2Psvr7x/FKf91H7X9h/RpfcYY84+IyPxuXwd4m33pkNT9guO6nPSF/2b+8T/ioMIqvHtOY8XfZgFwyDGnc/LvXmLV+ccx5i9rWHLOB1n8x8fKXLExxvSchcIeOu6cS9n4ySdQHA5+4lxevnMK25s2Ekum+Oh3fol393+iIshX/4Onb/sKnlcsd8nGGPMPWSjshbFHvZ/klS8xb9AnOa55Bu5dx/HXh26lkM9x1Gn/xJH/+zRvThjCqAde5NlPnUrzhtfLXbIxxrwnC4W9VDdgMCdc8TPWffo5VifGc+Ly29nw3WNZNPNR6gcO5+xfPs/6r3yMYX/fxorzzmXB739R7pKNMWaXLBR6yejDJtL4739g4cn/D1c9jn7xiyz+7hm89tL/cuZV3yM+7YfkEy7xa7/DU9+cYiu6GWP2SRYKvUgchwlnfppBUxfy10O+xojsCo58/vOsufVoWt98lcOn/5Y3TjqIMb+ew6yzT2LZy0+Xu2RjjHkbOyS1hLKZThY/PY0Br0xjrLeKVqp4bej5tHq11E+bQVXa583zjuOD/+ce4snqcpdrjKkQ73VIqoVCH1DfZ9nc58j86f9yVPtsBGWucyTrXy1y+OJWNg+K0e+mb3L0GReWu1RjTAWwUNiHbHpzBaufvZuR63/HcN3M7OZ65OUU9e2w6kPjOfXmu+0Lb8aYkrJQ2Aep7/P3BTNp/euDDNv0HIv+nmDskgidcVh5wjCO/sp/cNixp5W7TGPMAchCYR9XyOd47aUZvPn8vciCtYxdJWRisOYIqJlwOP0aP8aIo05h0LAxiGPHBhhj9o6Fwn5EfZ+XfnsvWx74Ke9b2knRhbWHFWk8uJX6ZIT1sYNprx0Hg4+kbvTRDD/0WGrq+pe7bGPMfsRCYT+1aslsXvvRrYz6yxoiPqwb7NA6xuXIQVsZn+roetxW6tgSHU5H1Si8+oOJDhpL7bBD6TfoIOobhhKJxsq4F8aYfY2Fwn5uw+uLWfzw3URenMPwN9MArB8WZ9tRB1E1aggDpY3q9Doa8usZxNuXB/VVaJEaWp3+dET7k4sPoBivRxP1SLIeN9WPaHV/4jX9SdYOoLqugep+A0kkq8qxq8aYPmChcABZu3w+rz76UyJ/nMPwtUFAbK912HbYMJITj2PYiWcSiVXRsXkV+ZaN+O2bcdJbiGWaSOW3UlvcRrV2UCOZ93yfrEZpl2o6nWoybi3ZaB3FWB1evB6S/ZFUP9xkLU4shRtL4EbjRGJJIvEk0XiKZE0/qmr7kaqqtXkQY/YxFgoHqDeXzWX509PJzVvAgGWbqOsIlgNtqXHY9r4huIcdSsMxJzL2+A9SP3D4255bLOTpaN1GR0sz6bat5Nq3ku/YRrFzO35mO5Jpwcm1EMm3ES+0kfDaqPLaqdV2UpLrcY2eCp2SopMUWaeKghOj6MQpOjH88OK5cdSN47sJNBKHSBKiSSQSx4lX4SZqiCSriSbriKVqiVfVkqyuI5GqIZmqwY1EevXnasyBzkKhAvi+zxuvvMQbM2eQm7eA+uWb6d/qdW1v7h+hbfRAnPGHUDe+keFHnsDwccfs0XxDLpumvaWZdNs2ivkcxXyGYi6Dl8/g5bMU85146TY014Zm23BybTj5DiKFdlw/j+vniPh5IponogWimiOmeWLkiWuBuBR2q56sRslIghwJsk6CvJMk76YouFV4kRRetAo/Vg2xaiRRi5OoJZqqJZKsJZaqI1FdT6K6jlRNP6qq63Bcd7d/JqbvFPJZtry5jKY1y2hbvwY3FiNZN4Bk/QBSdQOp7jeImv6DcSMx8rk0hWyaQi5DIZ+hkMuQ62wj3bqVbMtWsm0tFNpbKba14hfyRKpridX3I9FvIKl+DVQNGExV7QDaW7bQsWUDnVs3kd3aRGFbM15rK0SjuNU1RGvriNbVEa/pR6KuP5F4Ar/ooerhe0X8YhF8H88rol4R3yuiXrANXxHXpW74aBpGH8aAoQfjuqX9oGOhUKGaN7zOqrnPs3XRXPxlK6ld3czAbW+t65CPwNaGBOmRA3BGj6R6zDj6HXwYQ8cdRf8hY3DKNOzjex75XIZcppNspoNsRyu5dBv5zlYKmXa8bDtetgPNd6L5TiTfiRQzOIU0bjFNxEsT89LEvTRxTZPUDFWaISY9W9OiUxN0SoqcJChIjELYs/GcBJ4Tw4sk8WJ1aKIekvW4Vf2JVvUnVtOfaDxFNJEiGg6jxeJJYokkyVSNDaP1UOvWjWx8fTHbVv+djrVvUFi/HjY3E2tuo2p7hrp2H2cf+LNVcMH1ev8EcgUX2mojpPslKfSrRjwPJ5PHzRWIZItEc0ViOZ/mc47nnFvv26P3sFAwXVq3bmTNkpdofu1vZFYsx1mzgZqNrQzY7r3tcZkYtPaPkxlUiz9kIJFBDSSGDqd22Gj6HzSOQaMOI1VdX56d2EP5XJZ0ewvp9hayna3kOlvId7ZSzLThZdrwc+1oth3Jt+PkO3CLaRwvi+vnifg5In6OqJ8nrhmqtYNqTeNIz/7/+CqkSZCWJDlJkHVS5N0U+Uh12JupDnoz8RokXoOTqMGNV+HGq4gkqogla4glq4klqoklq4gnq0imqkt2ZJnv+6Tbt9GyZS2tW9bS2byJ7LZm8u2tFNvb8NOd+J1p6Ewj2RwajUAijiSTSDKJk0zipqpAFT+fw8/n0FwezefRQgGyOUhncNNZnEyeaKZANFsk1emRyr39Z5p3obU+Qrp/FcWGOmRwA/Ghw6keMYr6EWPxiwUyLVvJtm4l39ZCoS2oEVUkGsWJxpBYDCcaw4nFiMSTxOrqSdYNJFnXn6r6gVT3G0w0nqRj+2bamjfSsW0TmW1N5Fq2UujoCB4/YBDVDcOoGzSCuoaRpGr6AdDZ1kz7ts10bt9CumUrmdZm/GIRx3ER10UcB3Ejwad/x8GJRHDct1+8Qo7W9avpXL+G/MaN6JZmos2tJFuyeFGHYiKCF4/iJ2P4yQQk4/Q/7YO8/8Kr9ujf10LB/EMdrVvZsOJvNK96jY7VK8mvW4ezsYnUljZqWvKkdjKN0JkQOmqjZOuSFPtVIwP64Q4cSGLQYCLJKiLJFNFkNdFUVfAHLVVNqrY/VXUDSdX0L3kXudR8z6O9a16mmWzbNrxCBi+XwS9k0EK261rynZDvwCl04BY6iXhposU0cT9Nwu8kpRlSmtntobOCumSJkZM4OYnT4cfZkIuxvQOK7UqkpYjTPe8FNLwWP/yk64FbVCJFJeIpsZxSlVFi3i7eNJSJQS7uUIg5uEUllveJFZTYe3TICi4UXcjHHPJxl0LCpZiI4qXi+Mk41FQRGTaU1MhR9DvoUAaPPZIBw8bu978r+xoLBbPX2lu2sHnNUravfZ2ODWvIbliP19SEbGslur2DZFuOmrYi8d1YdTQbDf6o5OMu+WSEYnUCrzqJ1qSQmhrculqi9f1J9BtIon8DVf0HUTtwGHUNw0lW1ZdteKuUctk06fZWMp1t5DPtZDvbaWteR3vTWtLbt5Br3YrX2YHf2YlkMkg4rBDNFKnbVmRgi+KG/6V9YHst5MPORFenRkEA3wHfBS+iqKv4LmhE0YgiccWN+0RjHomYT1XUozpaJBXxqXY9qlwfVyCvEbISp4iLj4OPQ953SKtD1nPwJAqROBpJQSyJRqrwIkk0kkTdeHhgQRyJJJBIHIkmEDeKRGI4kRiOG15HY0TiKaKJamKJJLFkNfFE0GMSxwFVdvwtUw0OuHAjUWKxhA3b7YSFgukTvu/T0bKFbRvfINvZRiGTJp/poJjppJhJU8h0UOxop9jRgZ9O46c7oTODZLI4nRkiHTni6QLxTJGq9Ft/3Ham6EA+KuRjQiHmUIy5eLEIXiyCRl00Gum6EI1ALAqOE1wERBxwBMSBiBsMMcRjOLE4TiyGE08gbgRxBBBEJHiOCKji5bN42Qx+Loefy+KHQyM4grguOM5b1+EfJfX9HT8o8BVVPxhSSachnQ1+Dtk8brZALJ0n0VmkutN/z59DNgqZpEsuFSEzpB4dPZyqceMZfPixHHTkSVTV/ONvu6vvk8tlSLe3kOloI5/twCvkKOazeIU8fiGLV8zj5zN4uTR+Ph3O56ShkMEpdIJ6oD6iHqI+hNeul8X1MkS9DDE/S8zPEtcsCXJEtUCcQo+H4PZUVqPkJUae4LogMfKSoOAkKLpJim4iCConFu5DEfE9HC0i4X7tGOLTaBUaDvG5iRpUBLwi6hfB97quEQeJxBA3FoZdDDcSw4nGcWOp8PDtFNFEFbFEkmg0EfxbEISb73sQ/m2OJ6tJVtcSjyd7LeDeKxSsT2Z6jeM41PYf0itnefV9n862ZtqaN9DWvJHOreEY7/atFFu347W1o7lcMDadzeHkCm9dMnncot/tEgyNiIJoeE3wydnxIeLv/b5DEFSOsluToEUHcjEJhmHiLsVEhGI8SnpIPR111WytryPSvz/xAQNJDBhE1YChVA8YTO2AodT0H0wsntrrusVxSCSrgi8sDhr+j5/Qi9T3yRfy5HMZ8tk0+VwGr1CgWMjhF3MUC3m8Yh4vn8PLpSnmM/i5TvxCJginQhbCngHiIN1f3PdQLwfFLFLMIV4Ox8vhehlcL0vEy5IothLLbyamOaKax8PFF7fr2sdFgZhmSWmalGZI7OYQX28pqkOGOJlwXmr9IZ/mxItv7PX3sVAw+yTHcaipH0RN/SCGHzKhpO/l+z7FQjb4o5TpJJ/rJJ/pxCsWgk/zGnyqx1eCEXkJJ3uDeZJ4soZ4srpr3Nv3/a7DED0t4hUKiBP0NFw3guAgroOIE0w0VvDwhjgOsXiCWDwBtf3KXU6PFPK54ICFjhaAoAfgurhuNJw4dlFVCrkMxUKOYj4fXmfxClkK2XR4+HYGL5/GL2TRQjhp171HigCKn8+g+U7IdyKFzreOsqsZXJL9s1AwFc9xHGLxVPCpu25gr7ye48TAzjl1QIrG4tQNGEzdgNL8US63yv2IYowx5l0sFIwxxnSxUDDGGNPFQsEYY0wXCwVjjDFdLBSMMcZ0sVAwxhjTxULBGGNMl/363Eci0gSs2cOnDwSae7Gc/Uml7rvtd2Wx/d61UarasLMN+3Uo7A0RmberE0Id6Cp1322/K4vt956x4SNjjDFdLBSMMcZ0qeRQuLfcBZRRpe677Xdlsf3eAxU7p2CMMebdKrmnYIwx5h0sFIwxxnSpyFAQkY+IyN9FZKWITC13PaUiItNEZIuIvNKtrb+IPCciK8Lr/WO5q90gIiNFZKaIvCYir4rI1WH7Ab3vIpIQkTkisijc75vD9jEi8nL4+/6IiByQq/+IiCsifxOR34X3D/j9FpHVIrJERBaKyLywba9+zysuFETEBe4CzgYOBz4jIoeXt6qSuQ/4yDvapgLPq+o44Pnw/oGmCFynqocDJwJXhP/GB/q+54AzVPVoYALwERE5Efge8ENVPQTYDnyhfCWW1NXA0m73K2W/T1fVCd2+m7BXv+cVFwrA8cBKVV2lqnngYeC8MtdUEqo6C9j2jubzgPvD2/cDn+jLmvqCqm5U1QXh7XaCPxTDOcD3XQMd4d1oeFHgDOCxsP2A228AERkBfBT4aXhfqID93oW9+j2vxFAYDqztdn9d2FYpBqvqxvD2JuDAXGg2JCKjgWOAl6mAfQ+HUBYCW4DngNeBFlUthg85UH/ffwT8O+CH9wdQGfutwLMiMl9ELgvb9ur3PNKb1Zn9i6qqiBywxySLSDXwa+AaVW0LPjwGDtR9V1UPmCAi9cDjwPjyVlR6IvIxYIuqzheR08pcTl/7gKquF5FBwHMisqz7xj35Pa/EnsJ6YGS3+yPCtkqxWUSGAoTXW8pcT0mISJQgEB5U1d+EzRWx7wCq2gLMBE4C6kVkxwfAA/H3fTJwroisJhgOPgP4MQf+fqOq68PrLQQfAo5nL3/PKzEU5gLjwiMTYsCngRllrqkvzQCmhLenAL8tYy0lEY4n/wxYqqo/6LbpgN53EWkIewiISBL4EMF8ykzgn8KHHXD7rapfV9URqjqa4P/zC6p6MQf4fotIlYjU7LgNfBh4hb38Pa/IbzSLyDkEY5AuME1VbytvRaUhIr8CTiM4le5m4EbgCWA6cBDBaccvVNV3Tkbv10TkA8CfgCW8Ncb8DYJ5hQN230XkKIKJRZfgA990Vb1FRA4m+ATdH/gb8M+qmitfpaUTDh/9m6p+7EDf73D/Hg/vRoCHVPU2ERnAXvyeV2QoGGOM2blKHD4yxhizCxYKxhhjulgoGGOM6WKhYIwxpouFgjHGmC4WCsbshIh44Zknd1x67eR5IjK6+5lrjdmX2GkujNm5jKpOKHcRxvQ16ykYsxvC89d/PzyH/RwROSRsHy0iL4jIYhF5XkQOCtsHi8jj4RoHi0Tk/eFLuSLyk3Ddg2fDbyAjIleF60AsFpGHy7SbpoJZKBizc8l3DB9d1G1bq6o2Av+X4JvxAHcC96vqUcCDwB1h+x3Ai+EaB8cCr4bt44C7VPUIoAX4ZNg+FTgmfJ2vlGbXjNk1+0azMTshIh2qWr2T9tUEC9msCk+6t0lVB4hIMzBUVQth+0ZVHSgiTcCI7qdXCE/n/Vy4CAoicgMQVdVbReRpoIPgdCRPdFsfwZg+YT0FY3af7uL27uh+Dh6Pt+b3PkqwMuCxwNxuZ/k0pk9YKBiz+y7qdv2X8PafCc7QCXAxwQn5IFgO8XLoWgCnblcvKiIOMFJVZwI3AHXAu3orxpSSfQoxZueS4QpmOzytqjsOS+0nIosJPu1/Jmy7Evi5iFwPNAGXhu1XA/eKyBcIegSXAxvZORf4ZRgcAtwRrotgTJ+xOQVjdkM4pzBRVZvLXYsxpWDDR8YYY7pYT8EYY0wX6ykYY4zpYqFgjDGmi4WCMcaYLhYKxhhjulgoGGOM6fL/AUy5poJiMi8PAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the loss curves\n",
    "pd.DataFrame(history.history).plot(figsize=(6, 4), xlabel=\"Epochs\", ylabel=\"Loss\", title='Loss Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wsto_sf8fXM-"
   },
   "source": [
    "## Model Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TrKgnaZpfmJI",
    "outputId": "acd18217-d775-4d3a-b901-e490d8628d6f"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"Dense_1\" is incompatible with the layer: expected axis -1 of input shape to have value 12, but received input with shape (32, 13)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 13), dtype=float32)\n  • training=False\n  • mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [64]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# View the first prediction\u001b[39;00m\n\u001b[0;32m      5\u001b[0m y_pred[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\keras\\src\\layers\\input_spec.py:227\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m axis, value \u001b[38;5;129;01min\u001b[39;00m spec\u001b[38;5;241m.\u001b[39maxes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m shape[axis] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m {\n\u001b[0;32m    224\u001b[0m             value,\n\u001b[0;32m    225\u001b[0m             \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    226\u001b[0m         }:\n\u001b[1;32m--> 227\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    228\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincompatible with the layer: expected axis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof input shape to have value \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut received input with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Check shape.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Sequential.call().\n\n\u001b[1mInput 0 of layer \"Dense_1\" is incompatible with the layer: expected axis -1 of input shape to have value 12, but received input with shape (32, 13)\u001b[0m\n\nArguments received by Sequential.call():\n  • inputs=tf.Tensor(shape=(32, 13), dtype=float32)\n  • training=False\n  • mask=None"
     ]
    }
   ],
   "source": [
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# View the first prediction\n",
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Boston_Housing_Price_Prediction_with_a_Deep_Neural_Network.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
